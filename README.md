# Gradient-Origin-Networks
In this project, we reproduced the results from paper Gradient Origin Networks, which introduces
a new type of generative model that learns a latent representation. We almost reached the same
conclusion that the new model converges faster than the variational autoencoders and possesses a
lower loss. We also tested the model on other datasets and the hyper parameters that might affect
the overall loss. However, we found some disagreements with the original paper that needs further
investigation.
